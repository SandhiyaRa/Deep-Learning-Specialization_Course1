{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a331110a",
   "metadata": {},
   "source": [
    "# sigmoid function, np.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9031ea",
   "metadata": {},
   "source": [
    "# Implement the basic Sigmoid function using the exp function from the math library:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69952dc2",
   "metadata": {},
   "source": [
    "# $sigmoid(x) = \\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cb276e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def basic_sigmoid(x):\n",
    "    s=1/(1+math.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2373dd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525741268224334"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_sigmoid(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a46db684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum = :[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "#With Numpy:\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([1,2,3])\n",
    "print(\"sum = :\" + str(x+3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddb50aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 54.59815003, 148.4131591 , 403.42879349])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= np.array([4,5,6])\n",
    "\n",
    "np.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "985666cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement the Sigmoid function using numpy:\n",
    "\n",
    "def sigmoid(z):\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1978700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525741268224334"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dddb9f",
   "metadata": {},
   "source": [
    "# Sigmoid Gradient:\n",
    "\n",
    "$$ \\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40480849",
   "metadata": {},
   "source": [
    "# sigmoid_derivative\n",
    "Implement the function sigmoid_grad() to compute the gradient of the sigmoid function with respect to its input x. The formula is: \n",
    "\n",
    "$$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e173d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid Gradient:\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s= sigmoid(z)\n",
    "    ds = s*(1-s)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1072e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19661193, 0.10499359, 0.04517666])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2,3])\n",
    "sigmoid_derivative(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee8b7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping Arrays:\n",
    "\n",
    "    #Argument:\n",
    "    #image -- a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    #Returns:\n",
    "    #v -- a vector of shape (length*height*depth, 1)\n",
    "\n",
    "def image2vector(image):\n",
    "    v = image.reshape(image.shape[0]*image.shape[1]*image.shape[2],1)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8211a3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.67826139],\n",
       "       [0.29380381],\n",
       "       [0.90714982],\n",
       "       [0.52835647],\n",
       "       [0.4215251 ],\n",
       "       [0.45017551],\n",
       "       [0.92814219],\n",
       "       [0.96677647],\n",
       "       [0.85304703],\n",
       "       [0.52351845],\n",
       "       [0.19981397],\n",
       "       [0.27417313],\n",
       "       [0.60659855],\n",
       "       [0.00533165],\n",
       "       [0.10820313],\n",
       "       [0.49978937],\n",
       "       [0.34144279],\n",
       "       [0.94630077]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[[ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "image2vector(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831db53d",
   "metadata": {},
   "source": [
    "# Normalizing rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a142a3a8",
   "metadata": {},
   "source": [
    "# <a name='1-4'></a>\n",
    "###  Normalizing rows\n",
    "\n",
    "Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to $ \\frac{x}{\\| x\\|} $ (dividing each row vector of x by its norm).\n",
    "\n",
    "For example, if \n",
    "$$x = \\begin{bmatrix}\n",
    "        0 & 3 & 4 \\\\\n",
    "        2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}\\tag{3}$$ \n",
    "then \n",
    "$$\\| x\\| = \\text{np.linalg.norm(x, axis=1, keepdims=True)} = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}\\tag{4} $$\n",
    "and\n",
    "$$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}\\tag{5}$$ \n",
    "\n",
    "Note that you can divide matrices of different sizes and it works fine: this is called broadcasting \n",
    "\n",
    "With `keepdims=True` the result will broadcast correctly against the original x.\n",
    "\n",
    "`axis=1` means you are going to get the norm in a row-wise manner. If you need the norm in a column-wise way, you would need to set `axis=0`. \n",
    "\n",
    "\n",
    "\n",
    "<a name='ex-6'></a>\n",
    "###  normalize_rows\n",
    "Implement normalizeRows() to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (meaning length 1).\n",
    "\n",
    "**Note**: Don't try to use `x /= x_norm`. For the matrix division numpy must broadcast the x_norm, which is not supported by the operant `/=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aac07475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeRows(x):\n",
    "   \n",
    "    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n",
    "    x = x/x_norm\n",
    "    \n",
    "    print(x.shape, x_norm.shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f1b7aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3) (2, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.6       , 0.8       ],\n",
       "       [0.13736056, 0.82416338, 0.54944226]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = np.array([\n",
    "    [0, 3, 4],\n",
    "    [1, 6, 4]])\n",
    "\n",
    "normalizeRows(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac2aaec",
   "metadata": {},
   "source": [
    "# <a name='ex-7'></a>\n",
    "### Softmax\n",
    "\n",
    "Softmax function using numpy. You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes. You will learn more about softmax in the second course of this specialization.\n",
    "\n",
    "**Instructions**:\n",
    "- $\\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{,     }$\n",
    "\n",
    "\\begin{align*}\n",
    " softmax(x) &= softmax\\left(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}\\right) \\\\&= \\begin{bmatrix}\n",
    "    \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} \n",
    "\\end{align*}\n",
    "\n",
    "- $\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n} \\text{,  $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }$  \n",
    "\n",
    "\\begin{align*}\n",
    "softmax(x) &= softmax\\begin{bmatrix}\n",
    "            x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "            x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "            \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "            \\end{bmatrix} \\\\ \\\\&= \n",
    " \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} \\\\ \\\\ &= \\begin{pmatrix}\n",
    "    softmax\\text{(first row of x)}  \\\\\n",
    "    softmax\\text{(second row of x)} \\\\\n",
    "    \\vdots  \\\\\n",
    "    softmax\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a40be0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(x):\n",
    "    \n",
    "    # Apply exp() element-wise to x. Use np.exp(...).\n",
    "    x_exp = np.exp(x)\n",
    "\n",
    "    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n",
    "    x_sum = np.sum(x_exp, axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n",
    "    s = x_exp/x_sum\n",
    "\n",
    "  \n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afa5f4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.80897665e-01, 8.94462891e-04, 1.79657674e-02, 1.21052389e-04,\n",
       "        1.21052389e-04],\n",
       "       [8.78679856e-01, 1.18916387e-01, 8.01252314e-04, 8.01252314e-04,\n",
       "        8.01252314e-04]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = np.array([\n",
    "    [9, 2, 5, 0, 0],\n",
    "    [7, 5, 0, 0 ,0]])\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07116bf1",
   "metadata": {},
   "source": [
    "# <a name='2-1'></a>\n",
    "### Implementing the L1 and L2 loss functions\n",
    "\n",
    "<a name='ex-8'></a>\n",
    "###  L1 \n",
    "Implement the numpy vectorized version of the L1 loss. You may find the function abs(x) (absolute value of x) useful.\n",
    "\n",
    "**Reminder**:\n",
    "- The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions ($ \\hat{y} $) are from the true values ($y$). In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost.\n",
    "- L1 loss is defined as:\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^{m-1}|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c738f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def L1(yhat, y):\n",
    "        \n",
    "    loss= np.sum(np.abs(y-yhat))\n",
    "       \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d815ea6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "\n",
    "L1(yhat,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ac784b",
   "metadata": {},
   "source": [
    "# <a name='ex-9'></a>\n",
    "### L2\n",
    "Implementing the numpy vectorized version of the L2 loss. There are several way of implementing the L2 loss but you may find the function np.dot() useful. As a reminder, if $x = [x_1, x_2, ..., x_n]$, then `np.dot(x,x)` = $\\sum_{j=0}^n x_j^{2}$. \n",
    "\n",
    "- L2 loss is defined as $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^{m-1}(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bbbc0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2(yhat, y):\n",
    "    \n",
    "    loss = np.sum(np.dot(y-yhat,y-yhat))\n",
    "       \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02f410a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "\n",
    "L2(yhat,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c8f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
